{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-24T11:47:30.350536Z",
     "start_time": "2021-02-24T11:47:30.315077Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import torch.nn.functional as F\n",
    "from torch import nn, optim\n",
    "\n",
    "sys.path.append('scripts/')\n",
    "import myutils\n",
    "import gpu_utils as gu\n",
    "import ptb_utils as au\n",
    "import misc_utils as mu\n",
    "import synth_models as sm\n",
    "from synthetic_data import SemiRandomSignalCoordinate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Models on Synthetic Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-24T13:26:30.892722Z",
     "start_time": "2021-02-24T13:26:30.862206Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_dataset(): \n",
    "    args = {\n",
    "        'input_dim': 10,\n",
    "        'num_train': 10_000,\n",
    "        'num_test': 1_000,\n",
    "        'batch_size': 250\n",
    "    }\n",
    "    \n",
    "    sd_tr = SemiRandomSignalCoordinate(args['input_dim'], 1, 1./args['input_dim'])\n",
    "    sd_te = SemiRandomSignalCoordinate(args['input_dim'], 1, 1./args['input_dim'])\n",
    "\n",
    "    tr_dl = sd_tr.get_dataloader(args['num_train'], args['batch_size'])\n",
    "    te_dl = sd_te.get_dataloader(args['num_test'], args['batch_size'])\n",
    "\n",
    "    return {\n",
    "        'obj': (sd_tr, sd_te),\n",
    "        'loaders': (tr_dl, te_dl)\n",
    "    }\n",
    "\n",
    "def get_model(device):\n",
    "    args = {\n",
    "        'input_dim': 10,\n",
    "        'num_classes': 2,\n",
    "        'width': 25_000, \n",
    "        'depth': 1, \n",
    "        'activation': nn.ReLU,\n",
    "        'lr': 0.1,\n",
    "        'weight_decay': 0.0,\n",
    "        'momentum': 0.0,\n",
    "        'decay_gap': 50,\n",
    "        'decay_factor': 0.75\n",
    "    }\n",
    "    \n",
    "    model = sm.get_fcn(args['input_dim'], args['width'], \n",
    "                       args['num_classes'], activation=args['activation'])\n",
    "    model = model.to(device)\n",
    "    \n",
    "    opt = optim.SGD(model.parameters(), lr=args['lr'], \n",
    "                    weight_decay=args['weight_decay'], \n",
    "                    momentum=args['momentum'])\n",
    "    \n",
    "    def lr_scheduler(epoch):\n",
    "        pow = epoch//args['decay_gap']\n",
    "        return max(0.01, args['decay_factor']**pow)\n",
    "\n",
    "    sched = optim.lr_scheduler.LambdaLR(opt, lr_scheduler)\n",
    "\n",
    "    return model, opt, sched\n",
    "\n",
    "\n",
    "train_max_epochs = 200\n",
    "n_iter = 0\n",
    "\n",
    "# Train without regularisation\n",
    "def standard_training(train_data, test_data, epsilon, data_bounds, device):\n",
    "    # setup attack\n",
    "    epsilon = 0.0  # make sure epsilon has right value and is not used from pref run\n",
    "    attack = au.Linf_PGD_Attack(epsilon, lr=0.25*epsilon, num_iter=0, loss_type='untargeted', \n",
    "                                rand_eps=0.0, num_classes=2, bounds=data_bounds, device=device)\n",
    "\n",
    "    # setup model\n",
    "    model, opt, sched = get_model(device)\n",
    "    \n",
    "    # train for 500 epochs\n",
    "    stats = myutils.pgd_adv_fit_model(model, opt, train_data, test_data, attack, \n",
    "                                    sch=sched, min_loss=0.0, max_epochs=train_max_epochs, \n",
    "                                    device=device, epoch_gap=10)\n",
    "    \n",
    "    return stats, model.cpu()\n",
    "\n",
    "# Train with L1 gradient\n",
    "def l1_gradient_training(train_data, test_data, epsilon, data_bounds, device):\n",
    "    # setup attack\n",
    "    epsilon = 0.45  # make sure epsilon has right value and is not used from pref run\n",
    "    attack = au.L1_PGD_Attack(epsilon, lr=0.25*epsilon, num_iter=n_iter, loss_type='untargeted', \n",
    "                              rand_eps=0.0, num_classes=2, bounds=data_bounds, device=device)\n",
    "\n",
    "    # setup model\n",
    "    model, opt, sched = get_model(device)\n",
    "    \n",
    "    # train for 500 epochs\n",
    "    stats = myutils.pgd_adv_fit_model(model, opt, train_data, test_data, attack, \n",
    "                                    sch=sched, min_loss=0.0, max_epochs=train_max_epochs, \n",
    "                                    device=device, epoch_gap=10)\n",
    "    \n",
    "    return stats, model.cpu()\n",
    "\n",
    "\n",
    "# Train with L1 weight\n",
    "def l1_weight_training(train_data, test_data, epsilon, data_bounds, device):\n",
    "    # setup attack\n",
    "    epsilon = 0.0  # make sure epsilon has right value and is not used from pref run\n",
    "    attack = au.L1_PGD_Attack(epsilon, lr=0.25*epsilon, num_iter=n_iter, loss_type='untargeted', \n",
    "                              rand_eps=0.0, num_classes=2, bounds=data_bounds, device=device)\n",
    "\n",
    "    # setup model\n",
    "    model, opt, sched = get_model(device)\n",
    "    \n",
    "    # train for 500 epochs\n",
    "    stats = myutils.pgd_adv_fit_model_l1(model, opt, train_data, test_data, attack, \n",
    "                                    sch=sched, min_loss=0.0, max_epochs=train_max_epochs, \n",
    "                                    device=device, epoch_gap=10)\n",
    "    \n",
    "    return stats, model.cpu()\n",
    "\n",
    "# Train with L1 gradient and L1 weight\n",
    "def l1_gradient_l1_weight_training(train_data, test_data, epsilon, data_bounds, device):\n",
    "    # setup attack\n",
    "    epsilon = 0.45  # make sure epsilon has right value and is not used from pref run\n",
    "    attack = au.L1_PGD_Attack(epsilon, lr=0.25*epsilon, num_iter=n_iter, loss_type='untargeted', \n",
    "                              rand_eps=0.0, num_classes=2, bounds=data_bounds, device=device)\n",
    "\n",
    "    # setup model\n",
    "    model, opt, sched = get_model(device)\n",
    "    \n",
    "    # train for 500 epochs\n",
    "    stats = myutils.pgd_adv_fit_model_l1(model, opt, train_data, test_data, attack, \n",
    "                                    sch=sched, min_loss=0.0, max_epochs=train_max_epochs, \n",
    "                                    device=device, epoch_gap=10)\n",
    "    \n",
    "    return stats, model.cpu()\n",
    "\n",
    "\n",
    "# Train with L2 gradient\n",
    "def l2_gradient_training(train_data, test_data, epsilon, data_bounds, device):\n",
    "    # setup attack\n",
    "    epsilon = 0.45  # make sure epsilon has right value and is not used from pref run\n",
    "    attack = au.L2_PGD_Attack(epsilon, lr=0.25*epsilon, num_iter=n_iter, loss_type='untargeted', \n",
    "                              rand_eps=0.0, num_classes=2, bounds=data_bounds, device=device)\n",
    "\n",
    "    # setup model\n",
    "    model, opt, sched = get_model(device)\n",
    "    \n",
    "    # train for 500 epochs\n",
    "    stats = myutils.pgd_adv_fit_model(model, opt, train_data, test_data, attack, \n",
    "                                    sch=sched, min_loss=0.0, max_epochs=train_max_epochs, \n",
    "                                    device=device, epoch_gap=10)\n",
    "    \n",
    "    return stats, model.cpu()\n",
    "\n",
    "# Train with L2 weight\n",
    "def l2_weight_training(train_data, test_data, epsilon, data_bounds, device):\n",
    "    # setup attack\n",
    "    epsilon = 0.0  # make sure epsilon has right value and is not used from pref run\n",
    "    attack = au.L2_PGD_Attack(epsilon, lr=0.25*epsilon, num_iter=n_iter, loss_type='untargeted', \n",
    "                              rand_eps=0.0, num_classes=2, bounds=data_bounds, device=device)\n",
    "\n",
    "    # setup model\n",
    "    model, opt, sched = get_model(device)\n",
    "    \n",
    "    # train for 500 epochs\n",
    "    stats = myutils.pgd_adv_fit_model_l2(model, opt, train_data, test_data, attack, \n",
    "                                    sch=sched, min_loss=0.0, max_epochs=train_max_epochs, \n",
    "                                    device=device, epoch_gap=10)\n",
    "    \n",
    "    return stats, model.cpu()\n",
    "\n",
    "\n",
    "# Train with L2 gradient and L2 weight\n",
    "def l2_gradient_l2_weight_training(train_data, test_data, epsilon, data_bounds, device):\n",
    "    # setup attack\n",
    "    epsilon = 0.45  # make sure epsilon has right value and is not used from pref run\n",
    "    attack = au.L2_PGD_Attack(epsilon, lr=0.25*epsilon, num_iter=n_iter, loss_type='untargeted', \n",
    "                              rand_eps=0.0, num_classes=2, bounds=data_bounds, device=device)\n",
    "\n",
    "    # setup model\n",
    "    model, opt, sched = get_model(device)\n",
    "    \n",
    "    # train for 500 epochs\n",
    "    stats = myutils.pgd_adv_fit_model_l2(model, opt, train_data, test_data, attack, \n",
    "                                    sch=sched, min_loss=0.0, max_epochs=train_max_epochs, \n",
    "                                    device=device, epoch_gap=10)\n",
    "    \n",
    "    return stats, model.cpu()\n",
    "\n",
    "# Train with L1 gradient and L2 weight\n",
    "def l1_gradient_l2_weight_training(train_data, test_data, epsilon, data_bounds, device):\n",
    "    # setup attack\n",
    "    epsilon = 0.45  # make sure epsilon has right value and is not used from pref run\n",
    "    attack = au.L1_PGD_Attack(epsilon, lr=0.25*epsilon, num_iter=n_iter, loss_type='untargeted', \n",
    "                              rand_eps=0.0, num_classes=2, bounds=data_bounds, device=device)\n",
    "\n",
    "    # setup model\n",
    "    model, opt, sched = get_model(device)\n",
    "    \n",
    "    # train for 500 epochs\n",
    "    stats = myutils.pgd_adv_fit_model_l2(model, opt, train_data, test_data, attack, \n",
    "                                    sch=sched, min_loss=0.0, max_epochs=train_max_epochs, \n",
    "                                    device=device, epoch_gap=10)\n",
    "    \n",
    "    return stats, model.cpu()\n",
    "\n",
    "# Train with L2 gradient and L1 weight\n",
    "def l2_gradient_l1_weight_training(train_data, test_data, epsilon, data_bounds, device):\n",
    "    # setup attack\n",
    "    epsilon = 0.45  # make sure epsilon has right value and is not used from pref run\n",
    "    attack = au.L2_PGD_Attack(epsilon, lr=0.25*epsilon, num_iter=n_iter, loss_type='untargeted', \n",
    "                              rand_eps=0.0, num_classes=2, bounds=data_bounds, device=device)\n",
    "\n",
    "    # setup model\n",
    "    model, opt, sched = get_model(device)\n",
    "    \n",
    "    # train for 500 epochs\n",
    "    stats = myutils.pgd_adv_fit_model_l1(model, opt, train_data, test_data, attack, \n",
    "                                    sch=sched, min_loss=0.0, max_epochs=train_max_epochs, \n",
    "                                    device=device, epoch_gap=10)\n",
    "    \n",
    "    return stats, model.cpu()\n",
    "\n",
    "# Train with L_inf gradient\n",
    "def linf_gradient_training(train_data, test_data, epsilon, data_bounds, device):\n",
    "    # setup attack\n",
    "    epsilon = 0.45  # make sure epsilon has right value and is not used from pref run\n",
    "    attack = au.Linf_PGD_Attack(epsilon, lr=0.25*epsilon, num_iter=0, loss_type='untargeted', \n",
    "                                rand_eps=0.0, num_classes=2, bounds=data_bounds, device=device)\n",
    "\n",
    "    # setup model\n",
    "    model, opt, sched = get_model(device)\n",
    "    \n",
    "    # train for 500 epochs\n",
    "    stats = myutils.pgd_adv_fit_model(model, opt, train_data, test_data, attack, \n",
    "                                    sch=sched, min_loss=0.0, max_epochs=train_max_epochs, \n",
    "                                    device=device, epoch_gap=10)\n",
    "    \n",
    "    return stats, model.cpu()\n",
    "\n",
    "# Train with L_inf weight\n",
    "def linf_weight_training(train_data, test_data, epsilon, data_bounds, device):\n",
    "    # setup attack\n",
    "    epsilon = 0.0  # make sure epsilon has right value and is not used from pref run\n",
    "    attack = au.Linf_PGD_Attack(epsilon, lr=0.25*epsilon, num_iter=0, loss_type='untargeted', \n",
    "                                rand_eps=0.0, num_classes=2, bounds=data_bounds, device=device)\n",
    "\n",
    "    # setup model\n",
    "    model, opt, sched = get_model(device)\n",
    "    \n",
    "    # train for 500 epochs\n",
    "    stats = myutils.pgd_adv_fit_model_linf(model, opt, train_data, test_data, attack, \n",
    "                                    sch=sched, min_loss=0.0, max_epochs=train_max_epochs, \n",
    "                                    device=device, epoch_gap=10)\n",
    "    \n",
    "    return stats, model.cpu()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-24T13:26:33.719151Z",
     "start_time": "2021-02-24T13:26:32.251647Z"
    }
   },
   "outputs": [],
   "source": [
    "# visualize subsample of dataset\n",
    "# create new dataset\n",
    "dataset = get_dataset()\n",
    "\n",
    "bounds = dataset['obj'][0].bounds\n",
    "train_data, test_data = dataset['loaders']\n",
    "\n",
    "fig, ax = plt.subplots(1,1,figsize=(6,4))\n",
    "ax = sns.heatmap(dataset['obj'][1].X[:40], linewidth=0.1)\n",
    "myutils.update_ax(ax, r'Synthetic Dataset: $\\{(y\\cdot e_j, y)\\}^{n}_1$', 'Coordinates', 'Data points',  \n",
    "                legend_loc=False, despine=False, title_fs=20, label_fs=18)\n",
    "                \n",
    "ax.set_xticklabels(np.arange(1,11))\n",
    "\n",
    "#fig.savefig('../plots/data.pdf', dpi=fig.dpi, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Standard Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-24T13:27:40.234760Z",
     "start_time": "2021-02-24T13:26:34.515205Z"
    }
   },
   "outputs": [],
   "source": [
    "device = gu.get_device(0) # change 0 to None if cpu\n",
    "epsilon = 0.0 # standard training\n",
    "\n",
    "std_stats, std_model = standard_training(train_data, test_data, epsilon, bounds, device)\n",
    "torch.save(std_model.state_dict(), \"../pretrained_models/std_model.t7\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train $L_1$ Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = gu.get_device(0) # change 0 to None if cpu\n",
    "epsilon = None  # gets changed accordingly in the training functions\n",
    "\n",
    "l1_weight_stats, l1_weight_model = l1_weight_training(train_data, test_data, epsilon, bounds, device)\n",
    "torch.save(l1_weight_model.state_dict(), \"../pretrained_models/l1_weight.t7\")\n",
    "\n",
    "l1_gradient_stats, l1_gradient_model = l1_gradient_training(train_data, test_data, epsilon, bounds, device)\n",
    "torch.save(l1_gradient_model.state_dict(), \"../pretrained_models/l1_gradient.t7\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train $L_2$ Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = gu.get_device(0) # change 0 to None if cpu\n",
    "\n",
    "l2_weight_stats, l2_weight_model = l2_weight_training(train_data, test_data, epsilon, bounds, device)\n",
    "torch.save(l2_weight_model.state_dict(), \"../pretrained_models/l2_weight.t7\")\n",
    "\n",
    "l2_gradient_stats, l2_gradient_model = l2_gradient_training(train_data, test_data, epsilon, bounds, device)\n",
    "torch.save(l2_gradient_model.state_dict(), \"../pretrained_models/l2_gradient.t7\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train $L_\\infty$ Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-24T13:33:25.510779Z",
     "start_time": "2021-02-24T13:27:40.236284Z"
    }
   },
   "outputs": [],
   "source": [
    "device = gu.get_device(0) # change 0 to None if cpu\n",
    "\n",
    "linf_gradient_stats, linf_gradient_model = linf_gradient_training(train_data, test_data, epsilon, bounds, device)\n",
    "torch.save(linf_gradient_model.state_dict(), \"../pretrained_models/l_inf_gradient.t7\")\n",
    "\n",
    "linf_weight_stats, linf_weight_model = linf_weight_training(train_data, test_data, epsilon, bounds, device)\n",
    "torch.save(linf_weight_model.state_dict(), \"../pretrained_models/l_inf_weight.t7\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = gu.get_device(0) # change 0 to None if cpu\n",
    "\n",
    "models_path = \"../pretrained_models\"\n",
    "\n",
    "std_model, _, _ = get_model(device)\n",
    "std_model.load_state_dict(torch.load(models_path + \"/std_model.t7\"))\n",
    "std_model.eval()\n",
    "\n",
    "l1_weight_model, _, _ = get_model(device)\n",
    "l1_weight_model.load_state_dict(torch.load(models_path + \"/l1_weight.t7\"))\n",
    "l1_weight_model.eval()\n",
    "\n",
    "l1_gradient_model, _, _ = get_model(device)\n",
    "l1_gradient_model.load_state_dict(torch.load(models_path + \"/l1_gradient.t7\"))\n",
    "l1_gradient_model.eval()\n",
    "\n",
    "l2_weight_model, _, _ = get_model(device)\n",
    "l2_weight_model.load_state_dict(torch.load(models_path + \"/l2_weight.t7\"))\n",
    "l2_weight_model.eval()\n",
    "\n",
    "l2_gradient_model, _, _ = get_model(device)\n",
    "l2_gradient_model.load_state_dict(torch.load(models_path + \"/l2_gradient.t7\"))\n",
    "l2_gradient_model.eval()\n",
    "\n",
    "linf_weight_model, _, _ = get_model(device)\n",
    "linf_weight_model.load_state_dict(torch.load(models_path + \"/l_inf_weight.t7\"))\n",
    "linf_weight_model.eval()\n",
    "\n",
    "linf_gradient_model, _, _ = get_model(device)\n",
    "linf_gradient_model.load_state_dict(torch.load(models_path + \"/l_inf_gradient.t7\"))\n",
    "linf_gradient_model.eval()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating Standard and Robust Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-24T13:33:52.813127Z",
     "start_time": "2021-02-24T13:33:50.857303Z"
    }
   },
   "outputs": [],
   "source": [
    "device = gu.get_device(None)\n",
    "accs = {}\n",
    "epsilons = np.linspace(0, 0.2, 20)\n",
    "\n",
    "models = {\n",
    "    r'Standard': std_model,\n",
    "    r'Weights L1': l1_weight_model,\n",
    "    r'Gradients L1': l1_gradient_model,\n",
    "    r'Weights L2': l2_weight_model,\n",
    "    r'Gradients L2': l2_gradient_model,\n",
    "    r'Weights Linf': linf_weight_model,\n",
    "    r'Gradients Linf': linf_gradient_model\n",
    "}\n",
    "\n",
    "labels = {\n",
    "    'Standard': r'standard model',\n",
    "    'Weights L1': r'weights $\\ell_1$ regularized',\n",
    "    'Gradients L1': r'gradients $\\ell_1$ regularized',\n",
    "    'Weights L2': r'weights $\\ell_2$ regularized',\n",
    "    'Gradients L2': r'gradients $\\ell_2$ regularized',\n",
    "    'Weights Linf': r'weights $\\ell_{\\infty}$ regularized',\n",
    "    'Gradients Linf': r'gradients $\\ell_{\\infty}$ regularized'\n",
    "}\n",
    "\n",
    "pgd_attack = lambda eps: au.Linf_PGD_Attack(eps, 0.25*eps, 8, 'untargeted', \n",
    "                                            rand_eps=0., num_classes=2, \n",
    "                                            bounds=bounds, device=device)\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    model = model.to(device)\n",
    "    accs[model_name] = mu.evaluate_robustness([model], test_data, pgd_attack, \n",
    "                                              epsilons, device, print_info=False)[0]\n",
    "    model = model.cpu()\n",
    "    del model\n",
    "    \n",
    "fig, ax = plt.subplots(1,1,figsize=(10,4))\n",
    "for model_name, model_accs in accs.items():\n",
    "    x, y = map(np.array, zip(*sorted(model_accs.items())))\n",
    "    # ax.plot(x, y, marker='o', ms=5, mfc='w', mew=2, lw=2, ls='--', label=labels[model_name])\n",
    "    ax.plot(x, y, ms=5, mfc='w', mew=2, lw=2, label=labels[model_name])\n",
    "# ax.axvline(0.45, color='red', ls='--', label=r'$\\epsilon=0.45$')\n",
    "myutils.update_ax(ax, r'Evaluating Accuracy and Robustness', \n",
    "                r'Perturbation Budget $\\epsilon$', r'Accuracy', ticks_fs=14, label_fs=18, title_fs=20)\n",
    "ax.grid()\n",
    "ax.legend(loc='best', ncol=1, fontsize=14, frameon=True, fancybox=True, prop={'size': 10})\n",
    "\n",
    "ax.set_ylim(-0.05,1.05)\n",
    "\n",
    "fig.savefig('../plots/synthetic_data_robustness.pdf', dpi=fig.dpi, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating Input Gradients\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-24T13:33:58.021882Z",
     "start_time": "2021-02-24T13:33:57.971177Z"
    }
   },
   "outputs": [],
   "source": [
    "# compute loss gradients w.r.t. input \n",
    "test_dataset = dataset['obj'][1]\n",
    "device = gu.get_device(0)\n",
    "\n",
    "grads = {}\n",
    "for model_name, model in models.items():\n",
    "    model = model.to(device)\n",
    "    grads[model_name] = test_dataset.get_input_gradients(model, device)\n",
    "    model = model.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-24T13:33:59.485979Z",
     "start_time": "2021-02-24T13:33:59.258301Z"
    }
   },
   "outputs": [],
   "source": [
    "# FIG: bottom-most input gradient coordinate highlights signal coordinate of data point in standard models\n",
    "print (\"Pr[bottom-most gradient attribution higlights signal]\")\n",
    "\n",
    "for model_name, G in grads.items():\n",
    "    tracker = []\n",
    "    for x, g in zip(test_dataset.X, G):\n",
    "        signal_coord = np.argsort(np.abs(x))[-1]\n",
    "        grad_coord = np.argsort(np.abs(g))[0]\n",
    "        tracker.append(float(signal_coord==grad_coord))\n",
    "    print (\"{} Model: {:.3f}\".format(model_name, np.mean(tracker)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-24T13:34:00.824125Z",
     "start_time": "2021-02-24T13:34:00.591379Z"
    }
   },
   "outputs": [],
   "source": [
    "# RG-FIG: top-most input gradient coordinate highlights signal coordinate of data point in robust models\n",
    "print (\"Pr[top-most gradient attribution higlights signal]\")\n",
    "\n",
    "for model_name, G in grads.items():\n",
    "    tracker = []\n",
    "    for x, g in zip(test_dataset.X, G):\n",
    "        signal_coord = np.argsort(np.abs(x))[-1]\n",
    "        grad_coord = np.argsort(np.abs(g))[-1]\n",
    "        tracker.append(float(signal_coord==grad_coord))\n",
    "    print (\"{} Model: {:.3f}\".format(model_name, np.mean(tracker)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-24T13:47:24.168583Z",
     "start_time": "2021-02-24T13:47:22.855685Z"
    }
   },
   "outputs": [],
   "source": [
    "# visualizing input gradients of standard and robust models\n",
    "fig, axs = plt.subplots(1,1,figsize=(6, 5))\n",
    "xticks = np.arange(1,11)\n",
    "\n",
    "\n",
    "def filter_dict(d, keys_list):\n",
    "    return {k: d[k] for k in keys_list if k in d}\n",
    "\n",
    "\n",
    "my_grads = filter_dict(grads, [\"Standard\"])\n",
    "\n",
    "\n",
    "for ax, (model_name, G) in zip([axs], my_grads.items()):\n",
    "    # normalize gradients to improve visualization\n",
    "    G = np.abs(G[:40])\n",
    "    G /= G.max(axis=1).reshape(-1, 1)\n",
    "\n",
    "    # plot gradients\n",
    "    ax = sns.heatmap(G, ax=ax, vmax=1, vmin=0.8, linewidth=0.01, yticklabels=False, xticklabels=xticks)\n",
    "    myutils.update_ax(ax, title=labels[model_name], xlabel=r'Coordinates', ylabel=r'Input Gradients', \n",
    "                    ticks_fs=15, label_fs=18, title_fs=22, despine=False, legend_loc=False)\n",
    "\n",
    "    ax.tick_params(which='both', axis='both', length=3)\n",
    "    ax.set_xticklabels(xticks, fontsize=15)\n",
    "    \n",
    "\n",
    "#fig.savefig('../plots/feature_inversion_synthetic_data.pdf', dpi=fig.dpi, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates all 8 plots next to each other\n",
    "# visualizing input gradients of standard and robust models\n",
    "fig, axs = plt.subplots(3,3,figsize=(10, 10))\n",
    "xticks = np.arange(1,11)\n",
    "\n",
    "axs_flattened = []\n",
    "for ax in axs:\n",
    "    for a in ax:\n",
    "        axs_flattened.append(a)\n",
    "\n",
    "axs = axs_flattened\n",
    "\n",
    "\n",
    "myutils.update_ax(axs[0], title=r'Input Data', xlabel=r'Coordinates', ylabel=r'Data Points',\n",
    "                ticks_fs=10, label_fs=10, title_fs=14, despine=False, legend_loc=False)\n",
    "\n",
    "for ax, (model_name, G) in zip(axs[0:], grads.items()):\n",
    "    # normalize gradients to improve visualization\n",
    "    G = np.abs(G[:40])\n",
    "    G /= G.max(axis=1).reshape(-1, 1)\n",
    "\n",
    "    # plot gradients\n",
    "    ax = sns.heatmap(G, ax=ax, vmax=1, vmin=0.8, linewidth=0.01, yticklabels=False, xticklabels=xticks)\n",
    "    myutils.update_ax(ax, title=labels[model_name], xlabel=r'Coordinates', ylabel=r'Input Gradients', \n",
    "                    ticks_fs=10, label_fs=10, title_fs=14, despine=False, legend_loc=False)\n",
    "\n",
    "    ax.tick_params(which='both', axis='both', length=3)\n",
    "    ax.set_xticklabels(xticks, fontsize=10)\n",
    "    \n",
    "sup = fig.suptitle('Input Gradients on Synthetic Data', y=1.04, fontsize=20)\n",
    "fig.tight_layout(pad=1.5)\n",
    "#fig.savefig('../plots/feature_inversion_synthetic_data_all.pdf', dpi=fig.dpi, bbox_inches='tight', bbox_extra_artists=[sup])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "input_grads",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "246px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
